name: Deploy to GitHub Pages

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      product_data_run_id:
        description: 'Run ID of the product data fetch workflow'
        required: false
        type: string
      broken_link_run_id:
        description: 'Run ID of the broken link check workflow'
        required: false
        type: string
      trigger_source:
        description: 'Source workflow that triggered this deployment'
        required: false
        default: 'manual'
        type: string

permissions:
  contents: read
  pages: write
  id-token: write
  actions: read

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # 30 minute timeout for deployment
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas jinja2

      - name: Collect artifacts from multiple workflows
        id: collect-artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let artifactsCollected = {
              brokenLinkCsvs: false,
              productExportCsv: false
            };

            console.log('Collecting artifacts from multiple workflow runs...');
            console.log(`Product data run ID: ${{ inputs.product_data_run_id }}`);
            console.log(`Broken link run ID: ${{ inputs.broken_link_run_id }}`);

            // Function to download artifacts from a specific run
            async function downloadArtifactsFromRun(runId, artifactName, outputFile) {
              try {
                const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: runId
                });

                const artifact = artifacts.data.artifacts.find(
                  a => a.name === artifactName && !a.expired
                );

                if (artifact) {
                  const download = await github.rest.actions.downloadArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: artifact.id,
                    archive_format: 'zip'
                  });

                  fs.writeFileSync(outputFile, Buffer.from(download.data));
                  console.log(`Downloaded ${artifactName} from run ${runId}`);
                  return true;
                }
                return false;
              } catch (error) {
                console.log(`Error downloading ${artifactName} from run ${runId}: ${error.message}`);
                return false;
              }
            }

            // Function to find recent successful runs
            async function findRecentSuccessfulRuns(workflowFile) {
              const runs = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: workflowFile,
                status: 'completed',
                conclusion: 'success',
                per_page: 10
              });
              return runs.data.workflow_runs;
            }

            // Try to download broken link CSVs
            let brokenLinkRunId = '${{ inputs.broken_link_run_id }}';
            if (brokenLinkRunId) {
              artifactsCollected.brokenLinkCsvs = await downloadArtifactsFromRun(
                brokenLinkRunId, 'broken-link-csvs', 'broken-link-csvs.zip'
              );
            }

            if (!artifactsCollected.brokenLinkCsvs) {
              console.log('Searching for recent broken link check runs...');
              const brokenLinkRuns = await findRecentSuccessfulRuns('broken-link-check.yml');
              for (const run of brokenLinkRuns) {
                artifactsCollected.brokenLinkCsvs = await downloadArtifactsFromRun(
                  run.id, 'broken-link-csvs', 'broken-link-csvs.zip'
                );
                if (artifactsCollected.brokenLinkCsvs) break;
              }
            }

            // Try to download product export CSV
            let productDataRunId = '${{ inputs.product_data_run_id }}';
            if (productDataRunId) {
              artifactsCollected.productExportCsv = await downloadArtifactsFromRun(
                productDataRunId, 'product-export-csv', 'product-export-csv.zip'
              );
            }

            if (!artifactsCollected.productExportCsv) {
              console.log('Searching for recent product data fetch runs...');
              const productDataRuns = await findRecentSuccessfulRuns('product-data-fetch.yml');
              for (const run of productDataRuns) {
                artifactsCollected.productExportCsv = await downloadArtifactsFromRun(
                  run.id, 'product-export-csv', 'product-export-csv.zip'
                );
                if (artifactsCollected.productExportCsv) break;
              }
            }

            console.log('Artifact collection summary:');
            console.log(`- Broken link CSVs: ${artifactsCollected.brokenLinkCsvs}`);
            console.log(`- Product export CSV: ${artifactsCollected.productExportCsv}`);

            core.setOutput('broken_link_csvs', artifactsCollected.brokenLinkCsvs);
            core.setOutput('product_export_csv', artifactsCollected.productExportCsv);

      - name: Extract and validate CSV files
        run: |
          echo "Extracting and validating CSV files..."

          # Extract broken link CSVs
          if [ -f "broken-link-csvs.zip" ]; then
            unzip -o broken-link-csvs.zip
            echo "‚úì Extracted broken link CSVs"
          else
            echo "‚ö†Ô∏è No broken link CSV artifacts found"
            # Create placeholder files
            echo "URL,Status,Response_Time,Error" > au_broken_links.csv
            echo "https://example.com,404,1.0,Not Found" >> au_broken_links.csv
            echo "URL,Status,Response_Time,Error" > nz_broken_links.csv
            echo "https://example.com,404,1.0,Not Found" >> nz_broken_links.csv
          fi

          # Extract product export CSV
          if [ -f "product-export-csv.zip" ]; then
            unzip -o product-export-csv.zip
            echo "‚úì Extracted product export CSV"
          else
            echo "‚ö†Ô∏è No product export CSV artifacts found"
            # Create placeholder file
            echo "SKU,ID,DETAIL" > product_export.csv
            echo "12345678,NOT_FOUND,No product data available" >> product_export.csv
          fi

          # Validate and report on CSV files
          echo "üìä CSV File Summary:"
          for csv_file in au_broken_links.csv nz_broken_links.csv product_export_*.csv; do
            if [ -f "$csv_file" ]; then
              lines=$(wc -l < "$csv_file")
              echo "  ‚úì $csv_file: $lines lines"
            else
              echo "  ‚úó $csv_file: missing"
            fi
          done

          # Clean up zip files
          rm -f *.zip

      - name: Install Python dependencies for report_generator.py
        run: |
          echo "üì¶ Installing Python dependencies..."
          pip3 install pandas plotly networkx || {
            echo "‚ùå Failed to install dependencies"
            exit 1
          }
          echo "‚úÖ Dependencies installed successfully"

      - name: Generate HTML Report using existing report_generator.py
        run: |
          echo "üé® Generating comprehensive HTML report using report_generator.py..."

          # Find the latest product export file
          PRODUCT_FILE=$(ls -t product_export*.csv 2>/dev/null | head -n1)
          if [ -z "$PRODUCT_FILE" ]; then
            echo "‚ö†Ô∏è No product export file found, creating empty one"
            echo "SKU,ID,DETAIL" > product_export.csv
            PRODUCT_FILE="product_export.csv"
          fi

          echo "üìä Using product file: $PRODUCT_FILE"
          echo "üìä AU broken links: $(wc -l < au_broken_links.csv 2>/dev/null || echo 0) lines"
          echo "üìä NZ broken links: $(wc -l < nz_broken_links.csv 2>/dev/null || echo 0) lines"
          echo "üìä Product data: $(wc -l < $PRODUCT_FILE 2>/dev/null || echo 0) lines"

          # Use the existing report_generator.py with proper arguments
          python3 report_generator.py \
            --au-csv au_broken_links.csv \
            --nz-csv nz_broken_links.csv \
            --product-csv "$PRODUCT_FILE" \
            --output-html index.html

          echo "‚úÖ Generated comprehensive HTML report using report_generator.py"

          # Verify the output file was created
          if [ -f "index.html" ]; then
            echo "‚úÖ index.html created successfully"
            echo "üìÑ File size: $(wc -c < index.html) bytes"
            echo "üìÑ Contains Product Availability: $(grep -c 'Product Availability' index.html || echo 0) occurrences"
            echo "üìÑ Contains discontinued-indicator: $(grep -c 'discontinued-indicator' index.html || echo 0) occurrences"
            echo "üìÑ Contains product-table: $(grep -c 'product-table' index.html || echo 0) occurrences"
            echo "üìÑ Contains toggleProductDetails: $(grep -c 'toggleProductDetails' index.html || echo 0) occurrences"
          else
            echo "‚ùå Failed to create index.html"
            exit 1
          fi

      - name: Prepare deployment directory with validation
        env:
          USER_SECRET: ${{ secrets.USER }}
        run: |
          echo "üìÅ Preparing deployment directory..."
          mkdir -p gh-pages-deploy

          # Validate and copy HTML report
          if [ -f "index.html" ]; then
            # Validate HTML file is not empty
            if [ -s "index.html" ]; then
              cp index.html gh-pages-deploy/
              echo "‚úì Copied HTML report ($(du -h index.html | cut -f1))"
            else
              echo "‚ùå HTML report is empty"
              exit 1
            fi
          else
            echo "‚ùå HTML report not found, creating fallback page..."
            cat > gh-pages-deploy/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Report Generation Failed</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; text-align: center; }
                  .error { color: #d32f2f; }
                  .info { color: #1976d2; }
              </style>
          </head>
          <body>
              <h1 class="error">‚ö†Ô∏è Report Generation Failed</h1>
              <p class="info">The automated report generation encountered an error.</p>
              <p>Please check the workflow logs for more details.</p>
              <p><a href="downloads.html">üì• Download available CSV files</a></p>
          </body>
          </html>
          EOF
            echo "üìù Created fallback HTML page"
          fi

          # Copy all CSV files for download
          for csv_file in *.csv; do
            if [ -f "$csv_file" ]; then
              cp "$csv_file" gh-pages-deploy/
              echo "‚úì Copied $csv_file"
            fi
          done

          # Create download links page
          cat > gh-pages-deploy/downloads.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>Download CSV Files</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  .download-link { display: block; margin: 10px 0; padding: 10px; background: #f0f0f0; text-decoration: none; border-radius: 5px; }
                  .download-link:hover { background: #e0e0e0; }
              </style>
          </head>
          <body>
              <h1>üì• Download CSV Files</h1>
              <p>Click the links below to download the CSV files:</p>
          EOF

          for csv_file in gh-pages-deploy/*.csv; do
            if [ -f "$csv_file" ]; then
              filename=$(basename "$csv_file")
              echo "<a href=\"$filename\" class=\"download-link\" download>üìÑ $filename</a>" >> gh-pages-deploy/downloads.html
            fi
          done

          echo "</body></html>" >> gh-pages-deploy/downloads.html

          # --- Authentication assets and page guard ---
          # Copy auth UI and logic
          if [ -f "auth.html" ]; then
            cp auth.html gh-pages-deploy/
            echo "‚úì Copied auth.html"
          else
            echo "‚ö†Ô∏è auth.html not found in repo root"
          fi

          if [ -f "auth.js" ]; then
            cp auth.js gh-pages-deploy/
            echo "‚úì Copied auth.js"
          else
            echo "‚ö†Ô∏è auth.js not found in repo root"
          fi

          # Create auth-config.json from secret if available; fallback to repo file or dev config
          if [ -n "$USER_SECRET" ]; then
            HASH=$(printf "%s" "$USER_SECRET" | sha256sum | cut -d' ' -f1)
            cat > gh-pages-deploy/auth-config.json <<'EOF'
{
  "REPO_SECRET_NAME": "USER",
  "EXPECTED_SECRET_HASH": "__HASH__",
  "SESSION_TIMEOUT": 86400000,
  "SESSION_STORAGE_KEY": "auth_session",
  "SUCCESS_REDIRECT": "index.html"
}
EOF
            sed -i "s/__HASH__/$HASH/" gh-pages-deploy/auth-config.json || true
            echo "‚úì Created auth-config.json from secret"
          elif [ -f "auth-config.json" ]; then
            cp auth-config.json gh-pages-deploy/
            echo "‚úì Copied existing auth-config.json"
          else
            cat > gh-pages-deploy/auth-config.json <<'EOF'
{
  "REPO_SECRET_NAME": "USER",
  "EXPECTED_SECRET_HASH": "",
  "SESSION_TIMEOUT": 86400000,
  "SESSION_STORAGE_KEY": "auth_session",
  "SUCCESS_REDIRECT": "index.html"
}
EOF
            echo "‚ö†Ô∏è Created dev auth-config.json (no expected hash)"
          fi

          # Create a lightweight guard script that enforces auth on deployed pages
          cat > gh-pages-deploy/guard.js <<'EOF'
(function(){try{var KEY='auth_session',TIMEOUT=86400000;var raw=localStorage.getItem(KEY),s=null;try{s=raw?JSON.parse(raw):null}catch(e){};var valid=s&&s.authenticated&&s.ts&&(Date.now()-s.ts)<TIMEOUT;var p=location.pathname;var isAuth=/\/auth\.html$/.test(p);if(!valid&&!isAuth){location.replace('auth.html');return;}document.addEventListener('DOMContentLoaded',function(){if(valid&&!isAuth){var a=document.createElement('a');a.href='auth.html?signout=1';a.textContent='Sign out';a.style='position:fixed;right:12px;top:12px;padding:6px 10px;background:#eee;border-radius:6px;color:#111;text-decoration:none;font:12px system-ui;z-index:9999;';document.body.appendChild(a);}});}catch(e){}})();
EOF
          echo "‚úì Created guard.js"

          # Inject guard.js into all HTML pages in deployment directory
          for f in gh-pages-deploy/*.html; do
            [ -f "$f" ] || continue
            sed -i '0,/<head>/s//<head><script src="guard.js"><\/script>/' "$f" || true
            echo "‚úì Injected guard into $(basename "$f")"
          done

          echo "‚úÖ Deployment directory prepared"

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: './gh-pages-deploy'

      - name: Deploy to GitHub Pages
        id: deployment
        continue-on-error: false
        uses: actions/deploy-pages@v4

      - name: Create deployment summary
        run: |
          echo "## üöÄ GitHub Pages Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Deployment Details:**" >> $GITHUB_STEP_SUMMARY
          echo "- Trigger Source: ${{ inputs.trigger_source }}" >> $GITHUB_STEP_SUMMARY
          echo "- Product Data Run ID: ${{ inputs.product_data_run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- Broken Link Run ID: ${{ inputs.broken_link_run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- Deployment Time: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifacts Collected:**" >> $GITHUB_STEP_SUMMARY
          echo "- Broken Link CSVs: ${{ steps.collect-artifacts.outputs.broken_link_csvs }}" >> $GITHUB_STEP_SUMMARY
          echo "- Product Export CSV: ${{ steps.collect-artifacts.outputs.product_export_csv }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Files Deployed:**" >> $GITHUB_STEP_SUMMARY

          if [ -d "gh-pages-deploy" ]; then
            for file in gh-pages-deploy/*; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                size=$(du -h "$file" | cut -f1)
                echo "- $filename ($size)" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**üåê Live Report:** ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY
          echo "**üì• Downloads:** ${{ steps.deployment.outputs.page_url }}downloads.html" >> $GITHUB_STEP_SUMMARY
name: Broken Link Check

on:
  schedule:
    - cron: '0 16 * * *'  # 3AM AEST (UTC+10) = 4PM UTC
  workflow_dispatch:
    inputs:
      skip_link_checks:
        description: 'Skip the time-consuming link checker steps and use artifacts from previous run'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  actions: write  # Required to trigger other workflows

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      TARGET_BRANCH: main  # Branch containing the crawler/report code to execute
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Retrieve existing DB from gh-pages
        uses: actions/checkout@v3
        with:
          ref: gh-pages
          path: gh-pages
          fetch-depth: 1

      - name: Restore DB if exists
        run: |
          if [ -f gh-pages/broken_links.db ]; then
            cp gh-pages/broken_links.db ./broken_links.db
            echo "Restored existing broken_links.db"
          else
            echo "No existing DB found; starting fresh."
          fi

      - name: Run AU Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python au_link_checker.py

      - name: Run NZ Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python nz_link_checker.py

      - name: Download CSV artifacts from previous run
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Get the most recent successful workflow run
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'broken-link-check.yml',
              status: 'completed',
              conclusion: 'success',
              per_page: 10
            });

            console.log(`Found ${runs.data.workflow_runs.length} successful runs`);

            let artifactDownloaded = false;

            // Try to find and download artifacts from recent successful runs
            for (const run of runs.data.workflow_runs) {
              console.log(`Checking run ${run.id} from ${run.created_at}`);

              try {
                const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: run.id
                });

                console.log(`Found ${artifacts.data.artifacts.length} artifacts in run ${run.id}`);

                // Look for the link-check-report artifact
                const linkCheckArtifact = artifacts.data.artifacts.find(
                  artifact => artifact.name === 'link-check-report' && !artifact.expired
                );

                if (linkCheckArtifact) {
                  console.log(`Found link-check-report artifact: ${linkCheckArtifact.id}`);

                  // Download the artifact
                  const download = await github.rest.actions.downloadArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: linkCheckArtifact.id,
                    archive_format: 'zip'
                  });

                  // Save the zip file
                  fs.writeFileSync('artifact.zip', Buffer.from(download.data));
                  console.log('Artifact downloaded successfully');
                  artifactDownloaded = true;
                  break;
                }
              } catch (error) {
                console.log(`Error processing run ${run.id}: ${error.message}`);
                continue;
              }
            }

            if (!artifactDownloaded) {
              console.log('No suitable artifacts found in recent successful runs');
              core.setFailed('Could not find CSV artifacts from previous successful runs');
            }

      - name: Extract CSV files from artifact
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        run: |
          if [ -f "artifact.zip" ]; then
            echo "Extracting CSV files from downloaded artifact..."
            unzip -j artifact.zip "*.csv" || echo "No CSV files found in artifact"

            # List extracted files
            echo "Files extracted:"
            ls -la *.csv 2>/dev/null || echo "No CSV files found"

            # Verify required CSV files exist
            if [ ! -f "au_link_check_results.csv" ] || [ ! -f "nz_link_check_results.csv" ]; then
              echo "Warning: Required CSV files not found in artifact"
              echo "Creating placeholder CSV files..."

              # Create minimal placeholder CSV files
              echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
              echo "https://example.com,200,0.5," >> au_link_check_results.csv

              echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
              echo "https://example.com,200,0.5," >> nz_link_check_results.csv
            else
              echo "Successfully extracted required CSV files from artifact"
            fi

            # Clean up
            rm -f artifact.zip
          else
            echo "No artifact file found, creating placeholder CSV files..."
            echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
            echo "https://example.com,200,0.5," >> au_link_check_results.csv

            echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
            echo "https://example.com,200,0.5," >> nz_link_check_results.csv
          fi

      - name: Verify and validate CSV files
        run: |
          echo "üîç Checking for required CSV files..."

          csv_validation_passed=true

          # Check AU CSV file
          if [ -f "au_link_check_results.csv" ]; then
            lines=$(wc -l < au_link_check_results.csv)
            echo "‚úì au_link_check_results.csv found ($lines lines)"

            # Validate CSV structure
            if [ $lines -lt 2 ]; then
              echo "‚ö†Ô∏è AU CSV file has insufficient data (less than 2 lines)"
              csv_validation_passed=false
            else
              # Check for required columns
              header=$(head -1 au_link_check_results.csv)
              if [[ "$header" == *"URL"* ]]; then
                echo "‚úì AU CSV has valid header structure"
              else
                echo "‚ö†Ô∏è AU CSV missing required columns"
                csv_validation_passed=false
              fi
            fi
          else
            echo "‚úó au_link_check_results.csv missing"
            csv_validation_passed=false
          fi

          # Check NZ CSV file
          if [ -f "nz_link_check_results.csv" ]; then
            lines=$(wc -l < nz_link_check_results.csv)
            echo "‚úì nz_link_check_results.csv found ($lines lines)"

            # Validate CSV structure
            if [ $lines -lt 2 ]; then
              echo "‚ö†Ô∏è NZ CSV file has insufficient data (less than 2 lines)"
              csv_validation_passed=false
            else
              # Check for required columns
              header=$(head -1 nz_link_check_results.csv)
              if [[ "$header" == *"URL"* ]]; then
                echo "‚úì NZ CSV has valid header structure"
              else
                echo "‚ö†Ô∏è NZ CSV missing required columns"
                csv_validation_passed=false
              fi
            fi
          else
            echo "‚úó nz_link_check_results.csv missing"
            csv_validation_passed=false
          fi

          if [ "$csv_validation_passed" = true ]; then
            echo "‚úÖ All CSV files are present and valid"
          else
            echo "‚ùå CSV validation failed - creating minimal placeholder files"

            # Create minimal valid CSV files as fallback
            if [ ! -f "au_link_check_results.csv" ] || [ $(wc -l < au_link_check_results.csv) -lt 2 ]; then
              echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
              echo "https://example.com.au,404,1.0,Placeholder - No data available" >> au_link_check_results.csv
              echo "üìù Created placeholder AU CSV file"
            fi

            if [ ! -f "nz_link_check_results.csv" ] || [ $(wc -l < nz_link_check_results.csv) -lt 2 ]; then
              echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
              echo "https://example.co.nz,404,1.0,Placeholder - No data available" >> nz_link_check_results.csv
              echo "üìù Created placeholder NZ CSV file"
            fi
          fi

      - name: Generate CSV files for downstream workflows
        run: |
          echo "Preparing CSV files for downstream workflows..."

          # Ensure we have the correct CSV file names for downstream workflows
          if [ -f "au_link_check_results.csv" ] && [ ! -f "au_broken_links.csv" ]; then
            cp au_link_check_results.csv au_broken_links.csv
            echo "‚úì Created au_broken_links.csv"
          fi

          if [ -f "nz_link_check_results.csv" ] && [ ! -f "nz_broken_links.csv" ]; then
            cp nz_link_check_results.csv nz_broken_links.csv
            echo "‚úì Created nz_broken_links.csv"
          fi

          # Verify final CSV files
          echo "Final CSV file verification:"
          for csv_file in au_broken_links.csv nz_broken_links.csv; do
            if [ -f "$csv_file" ]; then
              lines=$(wc -l < "$csv_file")
              echo "‚úì $csv_file: $lines lines"
            else
              echo "‚úó $csv_file: missing"
            fi
          done

      - name: Upload CSV artifacts
        uses: actions/upload-artifact@v4
        with:
          name: broken-link-csvs
          path: |
            au_broken_links.csv
            nz_broken_links.csv
          retention-days: 30

      - name: Enforce 60-day retention in broken_links.db
        run: |
          if [ ! -f broken_links.db ]; then
            echo "broken_links.db not found; nothing to prune"
            exit 0
          fi
          python - <<'PY'
from datetime import date, timedelta, datetime
import sqlite3
import os

db_path = 'broken_links.db'
if not os.path.exists(db_path):
    raise SystemExit('broken_links.db not found; skipping retention enforcement')

conn = sqlite3.connect(db_path)
cur = conn.cursor()
cutoff = date.today() - timedelta(days=60)

cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'broken_links_%'")
tables = [row[0] for row in cur.fetchall()]

dropped = 0
for table in tables:
    suffix = table.replace('broken_links_', '')
    try:
        table_date = datetime.strptime(suffix, '%Y_%m_%d').date()
    except ValueError:
        continue
    if table_date < cutoff:
        print(f"Dropping historical table {table} older than 60 days")
        cur.execute(f'DROP TABLE IF EXISTS {table}')
        dropped += 1

conn.commit()
conn.close()
print(f"Retention enforcement complete; dropped {dropped} tables")
PY

      - name: Upload broken_links.db artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: broken-links-db
          path: broken_links.db
          if-no-files-found: warn
          retention-days: 60

      - name: Trigger Product Data Fetch Workflow
        if: success()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            try {
              console.log('üöÄ Triggering product data fetch workflow...');
              console.log(`Run ID: ${context.runId}`);

              const result = await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'product-data-fetch.yml',
                ref: 'main',
                inputs: {
                  'source_run_id': context.runId.toString(),
                  'trigger_source': 'broken-link-check',
                  'max_products': '100',
                  'region_filter': 'ALL'
                }
              });

              console.log('‚úÖ Successfully triggered product data fetch workflow');
              console.log(`Status: ${result.status}`);

              // Add to step summary
              core.summary.addHeading('üîó Workflow Coordination');
              core.summary.addRaw('‚úÖ Successfully triggered product data fetch workflow');
              core.summary.addRaw(`üìã Run ID: ${context.runId}`);
              await core.summary.write();

            } catch (error) {
              console.log('‚ùå Failed to trigger product data fetch workflow');
              console.log(`Error: ${error.message}`);

              // Add error to step summary
              core.summary.addHeading('üîó Workflow Coordination');
              core.summary.addRaw('‚ùå Failed to trigger product data fetch workflow');
              core.summary.addRaw(`Error: ${error.message}`);
              core.summary.addRaw('üí° You can manually trigger the product data fetch workflow using the artifacts from this run');
              await core.summary.write();

              // Don't fail the entire workflow
              core.warning('Failed to trigger downstream workflow, but CSV artifacts are available for manual processing');
            }
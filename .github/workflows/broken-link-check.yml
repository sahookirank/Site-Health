name: Broken Link Check

on:
  schedule:
    - cron: '0 16 * * *'  # 3AM AEST (UTC+10) = 4PM UTC
  workflow_dispatch:

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      TARGET_BRANCH: main  # Branch containing the crawler/report code to execute
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Retrieve existing DB from gh-pages
        uses: actions/checkout@v3
        with:
          ref: gh-pages
          path: gh-pages
          fetch-depth: 1

      - name: Restore DB if exists
        run: |
          if [ -f gh-pages/broken_links.db ]; then
            cp gh-pages/broken_links.db ./broken_links.db
            echo "Restored existing broken_links.db"
          else
            echo "No existing DB found; starting fresh."
          fi

      - name: Run AU Link Checker
        run: python au_link_checker.py

      - name: Run NZ Link Checker
        run: python nz_link_checker.py

      - name: Fetch Product Data
        run: |
          # Trigger the product data fetch workflow
          curl -X POST \
            -H "Accept: application/vnd.github.v3+json" \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            https://api.github.com/repos/${{ github.repository }}/actions/workflows/product-data-fetch.yml/dispatches \
            -d '{"ref":"${{ github.ref }}","inputs":{"max_products":"100","region_filter":"ALL"}}'
          
          # Wait for the workflow to complete and download the artifact
          echo "Product data fetch workflow triggered. Using existing product export file if available."
          
          # Check if product export file exists, if not create a fallback
          if [ ! -f "product_export_*.csv" ]; then
            echo "No product export file found, creating fallback data..."
            python -c "
            import pandas as pd
            import json
            
            # Use existing product export file as fallback
            try:
                df = pd.read_csv('product_export_20250902_055919.csv')
                print('Using existing product export file')
            except FileNotFoundError:
                print('Creating minimal fallback data')
                sample_data = {
                    'SKU': ['FALLBACK001'],
                    'ID': ['fallback-id-1'],
                    'DETAIL': [json.dumps({'name': 'Fallback Product', 'attributes': {}})]
                }
                df = pd.DataFrame(sample_data)
                df.to_csv('product_export.csv', index=False)
            "
          fi

      - name: Generate Combined Report
        run: python report_generator.py --au-csv au_link_check_results.csv --nz-csv nz_link_check_results.csv --output-html combined_report.html

      - name: Prepare GitHub Pages deployment
        run: |
          mkdir -p gh-pages-deploy
          mv combined_report.html gh-pages-deploy/index.html
          cp broken_links.db gh-pages-deploy/ 2>/dev/null || echo "No broken_links.db to copy"
          cp *_link_check_results.csv gh-pages-deploy/ 2>/dev/null || echo "No CSV files to copy"

      - name: Upload HTML report artifact
        uses: actions/upload-artifact@v4
        with:
          name: link-check-report
          path: |
            gh-pages-deploy/index.html
            gh-pages-deploy/broken_links.db
            gh-pages-deploy/*.csv

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: gh-pages-deploy
          publish_branch: gh-pages
          force_orphan: true

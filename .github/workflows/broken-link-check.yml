name: Broken Link Check

on:
  schedule:
    - cron: '0 16 * * *'  # 3AM AEST (UTC+10) = 4PM UTC
  workflow_dispatch:
    inputs:
      skip_link_checks:
        description: 'Skip the time-consuming link checker steps and use artifacts from previous run'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pages: write
  id-token: write
  actions: write  # Required to trigger other workflows

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      TARGET_BRANCH: main  # Branch containing the crawler/report code to execute
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Retrieve existing DB from gh-pages
        uses: actions/checkout@v3
        with:
          ref: gh-pages
          path: gh-pages
          fetch-depth: 1

      - name: Restore DB if exists
        run: |
          if [ -f gh-pages/broken_links.db ]; then
            cp gh-pages/broken_links.db ./broken_links.db
            echo "Restored existing broken_links.db"
          else
            echo "No existing DB found; starting fresh."
          fi

      - name: Run AU Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python au_link_checker.py

      - name: Run NZ Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python nz_link_checker.py

      - name: Download CSV artifacts from previous run
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Get the most recent successful workflow run
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'broken-link-check.yml',
              status: 'completed',
              conclusion: 'success',
              per_page: 10
            });

            console.log(`Found ${runs.data.workflow_runs.length} successful runs`);

            let artifactDownloaded = false;

            // Try to find and download artifacts from recent successful runs
            for (const run of runs.data.workflow_runs) {
              console.log(`Checking run ${run.id} from ${run.created_at}`);

              try {
                const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: run.id
                });

                console.log(`Found ${artifacts.data.artifacts.length} artifacts in run ${run.id}`);

                // Look for the link-check-report artifact
                const linkCheckArtifact = artifacts.data.artifacts.find(
                  artifact => artifact.name === 'link-check-report' && !artifact.expired
                );

                if (linkCheckArtifact) {
                  console.log(`Found link-check-report artifact: ${linkCheckArtifact.id}`);

                  // Download the artifact
                  const download = await github.rest.actions.downloadArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: linkCheckArtifact.id,
                    archive_format: 'zip'
                  });

                  // Save the zip file
                  fs.writeFileSync('artifact.zip', Buffer.from(download.data));
                  console.log('Artifact downloaded successfully');
                  artifactDownloaded = true;
                  break;
                }
              } catch (error) {
                console.log(`Error processing run ${run.id}: ${error.message}`);
                continue;
              }
            }

            if (!artifactDownloaded) {
              console.log('No suitable artifacts found in recent successful runs');
              core.setFailed('Could not find CSV artifacts from previous successful runs');
            }

      - name: Extract CSV files from artifact
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        run: |
          if [ -f "artifact.zip" ]; then
            echo "Extracting CSV files from downloaded artifact..."
            unzip -j artifact.zip "*.csv" || echo "No CSV files found in artifact"

            # List extracted files
            echo "Files extracted:"
            ls -la *.csv 2>/dev/null || echo "No CSV files found"

            # Verify required CSV files exist
            if [ ! -f "au_link_check_results.csv" ] || [ ! -f "nz_link_check_results.csv" ]; then
              echo "Warning: Required CSV files not found in artifact"
              echo "Creating placeholder CSV files..."

              # Create minimal placeholder CSV files
              echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
              echo "https://example.com,200,0.5," >> au_link_check_results.csv

              echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
              echo "https://example.com,200,0.5," >> nz_link_check_results.csv
            else
              echo "Successfully extracted required CSV files from artifact"
            fi

            # Clean up
            rm -f artifact.zip
          else
            echo "No artifact file found, creating placeholder CSV files..."
            echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
            echo "https://example.com,200,0.5," >> au_link_check_results.csv

            echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
            echo "https://example.com,200,0.5," >> nz_link_check_results.csv
          fi

      - name: Verify CSV files exist
        run: |
          echo "Checking for required CSV files..."

          if [ -f "au_link_check_results.csv" ]; then
            echo "✓ au_link_check_results.csv found ($(wc -l < au_link_check_results.csv) lines)"
          else
            echo "✗ au_link_check_results.csv missing"
            exit 1
          fi

          if [ -f "nz_link_check_results.csv" ]; then
            echo "✓ nz_link_check_results.csv found ($(wc -l < nz_link_check_results.csv) lines)"
          else
            echo "✗ nz_link_check_results.csv missing"
            exit 1
          fi

          echo "All required CSV files are present"

      - name: Setup Product Database
        run: |
          # Check for existing database, create if needed
          if [ ! -f "product_availability.db" ]; then
            echo "No existing database found, creating new one"
            python -c "import sqlite3; conn = sqlite3.connect('product_availability.db'); cursor = conn.cursor(); cursor.execute('CREATE TABLE IF NOT EXISTS \"products-in-links\" (ID INTEGER PRIMARY KEY AUTOINCREMENT, SKU TEXT UNIQUE NOT NULL, DETAILS TEXT NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)'); cursor.execute('CREATE INDEX IF NOT EXISTS idx_sku ON \"products-in-links\" (SKU)'); conn.commit(); conn.close(); print('Database initialized')"
          else
            echo "Using existing product database"
          fi

      - name: Install Product Fetch Dependencies
        run: |
          pip install pandas requests python-dotenv

      - name: Run Product Data Fetch
        run: |
          # Use simple shell script with working curl commands
          echo "Starting product data fetch using working curl commands..."

          # Check for required environment variables
          if [ -z "$COMMERCETOOL_AUTH_TOKEN" ]; then
              echo "Error: COMMERCETOOL_AUTH_TOKEN not set"
              exit 1
          fi

          # Initialize database
          python3 -c "
          import sqlite3
          conn = sqlite3.connect('product_availability.db')
          cursor = conn.cursor()
          cursor.execute('CREATE TABLE IF NOT EXISTS \"products-in-links\" (ID INTEGER PRIMARY KEY AUTOINCREMENT, SKU TEXT UNIQUE NOT NULL, DETAILS TEXT NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)')
          cursor.execute('CREATE INDEX IF NOT EXISTS idx_sku ON \"products-in-links\" (SKU)')
          conn.commit()
          conn.close()
          print('Database initialized')
          "

          # Extract SKUs from CSV files
          echo "Extracting SKUs from CSV files..."
          python3 -c "
          import pandas as pd
          import re
          import os
          skus = set()
          csv_files = ['au_broken_links.csv', 'nz_broken_links.csv']
          for csv_file in csv_files:
              if os.path.exists(csv_file):
                  try:
                      df = pd.read_csv(csv_file)
                      for col in df.columns:
                          for value in df[col].astype(str):
                              sku_matches = re.findall(r'\b\d{8}\b', str(value))
                              skus.update(sku_matches)
                  except Exception as e:
                      print(f'Error processing {csv_file}: {e}')
          sku_list = sorted(list(skus))[:20]  # Limit to 20 SKUs for testing
          print(f'Found {len(sku_list)} SKUs to process')
          with open('skus_to_process.txt', 'w') as f:
              for sku in sku_list:
                  f.write(sku + '\n')
          "

          # Process each SKU using working curl commands
          echo "Processing SKUs using curl commands..."
          counter=0
          total_skus=$(wc -l < skus_to_process.txt)
          echo "Total SKUs to process: $total_skus"

          while IFS= read -r sku; do
              counter=$((counter + 1))
              echo "Processing SKU $counter/$total_skus: $sku"

              # Generate correlation ID
              correlation_id="mc/kmart-production/941dc8ba-fd2b-4be3-a375-2d5ef173394d/$(uuidgen | tr '[:upper:]' '[:lower:]')"

              # Search API call using working curl command
              search_response=$(curl -s -X POST "https://mc-api.australia-southeast1.gcp.commercetools.com/proxy/pim-search/kmart-production/search/products" \
                  -H "accept: application/json" \
                  -H "accept-language: en-GB,en-US;q=0.9,en;q=0.8" \
                  -H "content-type: application/json" \
                  -H "origin: https://mc.australia-southeast1.gcp.commercetools.com" \
                  -H "user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36" \
                  -H "x-application-id: __internal:products" \
                  -H "x-correlation-id: $correlation_id" \
                  -H "x-project-key: kmart-production" \
                  -H "x-user-agent: @commercetools/sdk-client Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 products/sdk/24.5.0 (+https://git.io/fjuyC; +mc@commercetools.com)" \
                  -H "Cache-Control: no-cache" \
                  -H "Authorization: Bearer $COMMERCETOOL_AUTH_TOKEN" \
                  -d "{\"limit\": 20, \"offset\": 0, \"query\": {\"or\": [{\"fullText\": {\"field\": \"name\", \"language\": \"en-AU\", \"value\": \"$sku\"}}, {\"fullText\": {\"field\": \"description\", \"language\": \"en-AU\", \"value\": \"$sku\"}}, {\"fullText\": {\"field\": \"slug\", \"language\": \"en-AU\", \"value\": \"$sku\"}}, {\"prefix\": {\"field\": \"key\", \"language\": \"en-AU\", \"value\": \"$sku\", \"caseInsensitive\": true}}, {\"prefix\": {\"field\": \"variants.sku\", \"value\": \"$sku\", \"caseInsensitive\": true}}, {\"prefix\": {\"field\": \"variants.key\", \"value\": \"$sku\", \"caseInsensitive\": true}}]}, \"enableTiebreaker\": false}")

              # Check if search was successful and extract product ID
              product_id=$(echo "$search_response" | python3 -c "import sys, json; data=json.load(sys.stdin); hits=data.get('hits', []); print(hits[0]['id'] if hits else '')")

              if [ -n "$product_id" ]; then
                  echo "Found product ID: $product_id for SKU: $sku"

                  # Generate new correlation ID for GraphQL
                  graphql_correlation_id="mc/kmart-production/941dc8ba-fd2b-4be3-a375-2d5ef173394d/$(uuidgen | tr '[:upper:]' '[:lower:]')"

                  # GraphQL API call using working curl command
                  graphql_response=$(curl -s -X POST "https://mc-api.australia-southeast1.gcp.commercetools.com/graphql" \
                      -H "accept: application/json" \
                      -H "accept-language: en-GB,en-US;q=0.9,en;q=0.8" \
                      -H "content-type: application/json" \
                      -H "origin: https://mc.australia-southeast1.gcp.commercetools.com" \
                      -H "user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36" \
                      -H "x-application-id: __internal:products" \
                      -H "x-correlation-id: $graphql_correlation_id" \
                      -H "x-graphql-operation-name: GeneralInfoTabQuery" \
                      -H "x-graphql-target: ctp" \
                      -H "x-project-key: kmart-production" \
                      -H "x-user-agent: apollo-client Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 products/application-shell/24.5.0 (+https://git.io/fjuyC; +support@commercetools.com)" \
                      -H "Cache-Control: no-cache" \
                      -H "Authorization: Bearer $COMMERCETOOL_AUTH_TOKEN" \
                      -d "{\"operationName\": \"GeneralInfoTabQuery\", \"variables\": {\"productId\": \"$product_id\", \"isProductAttributeEnabled\": false, \"locale\": \"en-AU\", \"enableLocaleLabelOptimization\": false}, \"query\": \"query GeneralInfoTabQuery(\\\$productId: String, \\\$isProductAttributeEnabled: Boolean!, \\\$enableLocaleLabelOptimization: Boolean!, \\\$locale: Locale!) { product(id: \\\$productId) { id createdAt lastModifiedAt version key masterData { staged { nameAllLocales { locale value } masterVariant { attributesRaw { name value } } } } } }\"}")

                  # Store the result in database
                  python3 -c "
          import sys, json, sqlite3
          try:
              response_data = json.loads('$graphql_response')
              if 'data' in response_data and response_data['data'].get('product'):
                  product = response_data['data']['product']
                  details = json.dumps(product)
                  conn = sqlite3.connect('product_availability.db')
                  cursor = conn.cursor()
                  cursor.execute('INSERT OR REPLACE INTO \"products-in-links\" (SKU, DETAILS) VALUES (?, ?)', ('$sku', details))
                  conn.commit()
                  conn.close()
                  print('Successfully stored product data for SKU: $sku')
              else:
                  print('No product data found for SKU: $sku')
          except Exception as e:
              print(f'Error storing data for SKU $sku: {e}')
          "
              else
                  echo "No product found for SKU: $sku"
              fi

              # Small delay between requests
              sleep 1
          done < skus_to_process.txt

          echo "Product data fetch completed!"
        env:
          COMMERCETOOL_AUTH_TOKEN: ${{ secrets.COMMERCETOOL_AUTH_TOKEN }}
          COMMERCETOOL_API_KEY: ${{ secrets.COMMERCETOOL_API_KEY }}
          COMMERCETOOL_PROJECT_KEY: ${{ secrets.COMMERCETOOL_PROJECT_KEY }}

      - name: Generate Product Data CSV
        run: |
          # Generate CSV from database
          python3 -c "
          import sqlite3
          import pandas as pd
          import json

          try:
              conn = sqlite3.connect('product_availability.db')
              df = pd.read_sql_query('SELECT SKU, DETAILS FROM \"products-in-links\"', conn)
              conn.close()

              if not df.empty:
                  # Parse DETAILS JSON and create expanded CSV
                  expanded_data = []
                  for _, row in df.iterrows():
                      try:
                          details = json.loads(row['DETAILS'])
                          expanded_data.append({
                              'SKU': row['SKU'],
                              'Product_ID': details.get('id', ''),
                              'Product_Key': details.get('key', ''),
                              'Version': details.get('version', ''),
                              'Created_At': details.get('createdAt', ''),
                              'Last_Modified': details.get('lastModifiedAt', ''),
                              'Details_JSON': row['DETAILS']
                          })
                      except Exception as e:
                          print(f'Error parsing details for SKU {row[\"SKU\"]}: {e}')
                          expanded_data.append({
                              'SKU': row['SKU'],
                              'Product_ID': 'ERROR',
                              'Product_Key': 'ERROR',
                              'Version': 'ERROR',
                              'Created_At': 'ERROR',
                              'Last_Modified': 'ERROR',
                              'Details_JSON': row['DETAILS']
                          })

                  expanded_df = pd.DataFrame(expanded_data)
                  expanded_df.to_csv('product_availability.csv', index=False)
                  print(f'Generated product_availability.csv with {len(expanded_df)} records')
              else:
                  print('No product data found in database')
                  # Create empty CSV
                  empty_df = pd.DataFrame(columns=['SKU', 'Product_ID', 'Product_Key', 'Version', 'Created_At', 'Last_Modified', 'Details_JSON'])
                  empty_df.to_csv('product_availability.csv', index=False)
          except Exception as e:
              print(f'Error generating CSV: {e}')
              # Create error CSV
              error_df = pd.DataFrame([{'SKU': 'ERROR', 'Product_ID': 'ERROR', 'Product_Key': 'ERROR', 'Version': 'ERROR', 'Created_At': 'ERROR', 'Last_Modified': 'ERROR', 'Details_JSON': str(e)}])
              error_df.to_csv('product_availability.csv', index=False)
          "
        env:
          COMMERCETOOL_AUTH_TOKEN: ${{ secrets.COMMERCETOOL_AUTH_TOKEN }}
          COMMERCETOOL_API_KEY: ${{ secrets.COMMERCETOOL_API_KEY }}
          COMMERCETOOL_PROJECT_KEY: ${{ secrets.COMMERCETOOL_PROJECT_KEY }}

      - name: Generate Combined Report
        run: |
          echo "Generating combined report..."
          python product_availability_ui.py
        continue-on-error: true

      - name: Prepare GitHub Pages deployment
        run: |
          mkdir -p gh-pages

          # Copy HTML reports
          if [ -f "combined_report.html" ]; then
            cp combined_report.html gh-pages/index.html
            echo "✅ Combined report copied to gh-pages/index.html"
          else
            echo "⚠️ Combined report not found, creating placeholder"
            echo "<html><body><h1>Report Generation Failed</h1><p>Please check the workflow logs.</p></body></html>" > gh-pages/index.html
          fi

          # Copy CSV files
          for csv_file in *.csv; do
            if [ -f "$csv_file" ]; then
              cp "$csv_file" gh-pages/
              echo "✅ Copied $csv_file to gh-pages/"
            fi
          done

          # Copy database file
          if [ -f "product_availability.db" ]; then
            cp product_availability.db gh-pages/
            echo "✅ Copied database to gh-pages/"
          fi

      - name: Upload HTML report artifact
        uses: actions/upload-artifact@v4
        with:
          name: html-report
          path: |
            combined_report.html
            *.csv
            product_availability.db
        continue-on-error: true

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        if: github.ref == 'refs/heads/main'
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./gh-pages
          force_orphan: true

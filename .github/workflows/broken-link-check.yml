name: Broken Link Check

on:
  schedule:
    - cron: '0 16 * * *'  # 3AM AEST (UTC+10) = 4PM UTC
  workflow_dispatch:
    inputs:
      skip_link_checks:
        description: 'Skip the time-consuming link checker steps and use artifacts from previous run'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pages: write
  id-token: write
  actions: write  # Required to trigger other workflows

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      TARGET_BRANCH: main  # Branch containing the crawler/report code to execute
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Retrieve existing DB from gh-pages
        uses: actions/checkout@v3
        with:
          ref: gh-pages
          path: gh-pages
          fetch-depth: 1

      - name: Restore DB if exists
        run: |
          if [ -f gh-pages/broken_links.db ]; then
            cp gh-pages/broken_links.db ./broken_links.db
            echo "Restored existing broken_links.db"
          else
            echo "No existing DB found; starting fresh."
          fi

      - name: Run AU Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python au_link_checker.py

      - name: Run NZ Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python nz_link_checker.py

      - name: Download CSV artifacts from previous run
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Get the most recent successful workflow run
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'broken-link-check.yml',
              status: 'completed',
              conclusion: 'success',
              per_page: 10
            });

            console.log(`Found ${runs.data.workflow_runs.length} successful runs`);

            let artifactDownloaded = false;

            // Try to find and download artifacts from recent successful runs
            for (const run of runs.data.workflow_runs) {
              console.log(`Checking run ${run.id} from ${run.created_at}`);

              try {
                const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  run_id: run.id
                });

                console.log(`Found ${artifacts.data.artifacts.length} artifacts in run ${run.id}`);

                // Look for the link-check-report artifact
                const linkCheckArtifact = artifacts.data.artifacts.find(
                  artifact => artifact.name === 'link-check-report' && !artifact.expired
                );

                if (linkCheckArtifact) {
                  console.log(`Found link-check-report artifact: ${linkCheckArtifact.id}`);

                  // Download the artifact
                  const download = await github.rest.actions.downloadArtifact({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    artifact_id: linkCheckArtifact.id,
                    archive_format: 'zip'
                  });

                  // Save the zip file
                  fs.writeFileSync('artifact.zip', Buffer.from(download.data));
                  console.log('Artifact downloaded successfully');
                  artifactDownloaded = true;
                  break;
                }
              } catch (error) {
                console.log(`Error processing run ${run.id}: ${error.message}`);
                continue;
              }
            }

            if (!artifactDownloaded) {
              console.log('No suitable artifacts found in recent successful runs');
              core.setFailed('Could not find CSV artifacts from previous successful runs');
            }

      - name: Extract CSV files from artifact
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        run: |
          if [ -f "artifact.zip" ]; then
            echo "Extracting CSV files from downloaded artifact..."
            unzip -j artifact.zip "*.csv" || echo "No CSV files found in artifact"

            # List extracted files
            echo "Files extracted:"
            ls -la *.csv 2>/dev/null || echo "No CSV files found"

            # Verify required CSV files exist
            if [ ! -f "au_link_check_results.csv" ] || [ ! -f "nz_link_check_results.csv" ]; then
              echo "Warning: Required CSV files not found in artifact"
              echo "Creating placeholder CSV files..."

              # Create minimal placeholder CSV files
              echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
              echo "https://example.com,200,0.5," >> au_link_check_results.csv

              echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
              echo "https://example.com,200,0.5," >> nz_link_check_results.csv
            else
              echo "Successfully extracted required CSV files from artifact"
            fi

            # Clean up
            rm -f artifact.zip
          else
            echo "No artifact file found, creating placeholder CSV files..."
            echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
            echo "https://example.com,200,0.5," >> au_link_check_results.csv

            echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
            echo "https://example.com,200,0.5," >> nz_link_check_results.csv
          fi

      - name: Verify CSV files exist
        run: |
          echo "Checking for required CSV files..."

          if [ -f "au_link_check_results.csv" ]; then
            echo "✓ au_link_check_results.csv found ($(wc -l < au_link_check_results.csv) lines)"
          else
            echo "✗ au_link_check_results.csv missing"
            exit 1
          fi

          if [ -f "nz_link_check_results.csv" ]; then
            echo "✓ nz_link_check_results.csv found ($(wc -l < nz_link_check_results.csv) lines)"
          else
            echo "✗ nz_link_check_results.csv missing"
            exit 1
          fi

          echo "All required CSV files are present"

      - name: Setup Product Database
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: |
          # Check for existing database, create if needed
          if [ ! -f "product_availability.db" ]; then
            echo "No existing database found, creating new one"
            cat > init_db.py << 'EOF'
import sqlite3
conn = sqlite3.connect('product_availability.db')
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS "products-in-links" (
        ID INTEGER PRIMARY KEY AUTOINCREMENT,
        SKU TEXT UNIQUE NOT NULL,
        DETAILS TEXT NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
''')
cursor.execute('CREATE INDEX IF NOT EXISTS idx_sku ON "products-in-links" (SKU)')
conn.commit()
conn.close()
print('Database initialized')
EOF
            python init_db.py
            rm init_db.py
          else
            echo "Using existing product database"
          fi

      - name: Install Product Fetch Dependencies
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: |
          pip install requests python-dotenv

      - name: Create Product Fetch Script
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: |
          cat > product_fetch_script.py << 'EOF'
          #!/usr/bin/env python3
          """
          Inline Product Data Fetcher for Broken Link Check Workflow
          """

          import os
          import sys
          import sqlite3
          import json
          import requests
          import pandas as pd
          import logging
          from datetime import datetime
          from typing import List, Dict, Optional
          import time

          # Configure logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger(__name__)

          class ProductFetcher:
              def __init__(self):
                  self.db_path = "product_availability.db"
                  self.base_url = "https://mc-api.australia-southeast1.gcp.commercetools.com"
                  self.search_endpoint = f"{self.base_url}/proxy/pim-search/kmart-production/search/products"
                  self.graphql_endpoint = f"{self.base_url}/graphql"

                  # Get secrets from environment
                  self.auth_token = os.getenv('COMMERCETOOL_AUTH_TOKEN')
                  self.project_key = 'kmart-production'

                  if not self.auth_token:
                      logger.warning("COMMERCETOOL_AUTH_TOKEN not found, will create fallback data")
                      self.auth_token = None

                  # Headers for API requests
                  if self.auth_token:
                      self.search_headers = {
                          "accept": "application/json",
                          "content-type": "application/json",
                          "authorization": f"Bearer {self.auth_token}",
                          "x-project-key": self.project_key,
                          "cache-control": "no-cache"
                      }

                      self.graphql_headers = {
                          "accept": "application/json",
                          "content-type": "application/json",
                          "authorization": f"Bearer {self.auth_token}",
                          "x-project-key": self.project_key,
                          "x-graphql-operation-name": "GeneralInfoTabQuery",
                          "x-graphql-target": "ctp",
                          "cache-control": "no-cache"
                      }

              def get_products_from_database(self, max_products: int = 100) -> List[Dict]:
                  """Get products from database."""
                  try:
                      conn = sqlite3.connect(self.db_path)
                      cursor = conn.cursor()

                      query = 'SELECT ID, SKU, DETAILS FROM "products-in-links" LIMIT ?'
                      cursor.execute(query, (max_products,))

                      rows = cursor.fetchall()
                      conn.close()

                      products = []
                      for row in rows:
                          try:
                              details = json.loads(row[2]) if row[2] else {}
                          except json.JSONDecodeError:
                              details = {"error": "Invalid JSON data"}

                          products.append({
                              "id": row[0],
                              "sku": row[1],
                              "details": details
                          })

                      logger.info(f"Retrieved {len(products)} products from database")
                      return products

                  except Exception as e:
                      logger.error(f"Error retrieving products from database: {e}")
                      return []

              def create_fallback_data(self) -> List[Dict]:
                  """Create fallback data when API is not available."""
                  logger.info("Creating fallback product data")
                  return [
                      {
                          "SKU": "FALLBACK001",
                          "ID": "fallback-id-1",
                          "DETAIL": json.dumps({"name": "Fallback Product", "attributes": {}})
                      }
                  ]

              def save_to_csv(self, results: List[Dict], filename: str = None):
                  """Save results to CSV file."""
                  if not filename:
                      timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                      filename = f"product_export_{timestamp}.csv"

                  try:
                      df = pd.DataFrame(results)
                      df.to_csv(filename, index=False)
                      logger.info(f"Saved {len(results)} records to {filename}")
                      return filename
                  except Exception as e:
                      logger.error(f"Error saving to CSV: {e}")
                      return None

              def run(self):
                  """Main execution function."""
                  max_products = int(os.getenv('MAX_PRODUCTS', '100'))

                  if not self.auth_token:
                      logger.info("No auth token available, creating fallback data")
                      results = self.create_fallback_data()
                  else:
                      # Try to get products from database
                      products = self.get_products_from_database(max_products)

                      if not products:
                          logger.warning("No products found in database, creating fallback data")
                          results = self.create_fallback_data()
                      else:
                          # For now, just use the database data as-is
                          # In a full implementation, you would fetch details from the API
                          results = []
                          for product in products[:max_products]:
                              results.append({
                                  "SKU": product["sku"],
                                  "ID": product["id"],
                                  "DETAIL": json.dumps(product["details"])
                              })

                  # Save to CSV
                  filename = self.save_to_csv(results)
                  if filename:
                      print(f"Product export completed: {filename}")
                      return True
                  else:
                      print("Product export failed")
                      return False

          if __name__ == "__main__":
              fetcher = ProductFetcher()
              success = fetcher.run()
              sys.exit(0 if success else 1)
          EOF

      - name: Run Product Data Fetch
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        env:
          COMMERCETOOL_AUTH_TOKEN: ${{ secrets.COMMERCETOOL_AUTH_TOKEN }}
          COMMERCETOOL_API_KEY: ${{ secrets.COMMERCETOOL_API_KEY }}
          COMMERCETOOL_PROJECT_KEY: ${{ secrets.COMMERCETOOL_PROJECT_KEY }}
          MAX_PRODUCTS: '100'
        run: |
          echo "Starting product data fetch..."
          python product_fetch_script.py

          # Ensure we have a product export file for the report
          if [ ! -f product_export_*.csv ]; then
            echo "No product export file generated, creating minimal fallback"
            cat > product_export_fallback.csv << 'EOF'
          SKU,ID,DETAIL
          FALLBACK001,fallback-id-1,"{""name"": ""Fallback Product"", ""attributes"": {}}"
          EOF
          fi

          echo "Product data fetch completed"

      - name: Fetch Product Data Fallback
        if: ${{ github.event_name == 'workflow_dispatch' && inputs.skip_link_checks }}
        run: |
          # When skipping link checks, also skip product fetch and use existing data
          echo "Skipping product data fetch, looking for existing product export files..."

          # Try to use existing product export file first
          if [ -f "product_export_20250902_055919.csv" ]; then
            echo "Using existing product export file"
            cp product_export_20250902_055919.csv product_export.csv
          else
            echo "Creating minimal fallback data"
            cat > product_export.csv << 'EOF'
          SKU,ID,DETAIL
          FALLBACK001,fallback-id-1,"{""name"": ""Fallback Product"", ""attributes"": {}}"
          EOF
          fi

      - name: Generate Combined Report
        run: python report_generator.py --au-csv au_link_check_results.csv --nz-csv nz_link_check_results.csv --output-html combined_report.html

      - name: Prepare GitHub Pages deployment
        run: |
          mkdir -p gh-pages-deploy
          mv combined_report.html gh-pages-deploy/index.html
          cp broken_links.db gh-pages-deploy/ 2>/dev/null || echo "No broken_links.db to copy"
          cp *_link_check_results.csv gh-pages-deploy/ 2>/dev/null || echo "No link check CSV files to copy"
          cp product_export*.csv gh-pages-deploy/ 2>/dev/null || echo "No product export CSV files to copy"

          echo "Files prepared for deployment:"
          ls -la gh-pages-deploy/

      - name: Upload HTML report artifact
        uses: actions/upload-artifact@v4
        with:
          name: link-check-report
          path: |
            gh-pages-deploy/index.html
            gh-pages-deploy/broken_links.db
            gh-pages-deploy/*.csv

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: gh-pages-deploy
          publish_branch: gh-pages
          force_orphan: true

name: Broken Link Check

on:
  schedule:
    - cron: '0 16 * * *'  # 3AM AEST (UTC+10) = 4PM UTC
  workflow_dispatch:
    inputs:
      skip_link_checks:
        description: 'Skip the time-consuming link checker steps and use artifacts from previous run'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pages: write
  id-token: write
  actions: write  # Required to trigger other workflows

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      TARGET_BRANCH: main  # Branch containing the crawler/report code to execute
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install requests python-dotenv

      - name: Retrieve existing DB from gh-pages
        uses: actions/checkout@v3
        with:
          ref: gh-pages
          path: gh-pages
          fetch-depth: 1

      - name: Restore DB if exists
        run: |
          if [ -f gh-pages/broken_links.db ]; then
            cp gh-pages/broken_links.db ./broken_links.db
            echo "Restored existing broken_links.db"
          else
            echo "No existing DB found; starting fresh."
          fi

      - name: Run AU Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python au_link_checker.py

      - name: Run NZ Link Checker
        if: ${{ github.event_name == 'schedule' || !inputs.skip_link_checks }}
        run: python nz_link_checker.py

      - name: Check for Existing CSV Files and Download if Missing
        run: |
          echo "Checking for existing AU and NZ CSV files..."

          # Check if CSV files already exist in workspace
          AU_EXISTS=false
          NZ_EXISTS=false

          if [ -f "au_link_check_results.csv" ]; then
            echo "✓ au_link_check_results.csv found in workspace"
            AU_EXISTS=true
          fi

          if [ -f "nz_link_check_results.csv" ]; then
            echo "✓ nz_link_check_results.csv found in workspace"
            NZ_EXISTS=true
          fi

          # If either file is missing, download from most recent successful run
          if [ "$AU_EXISTS" = false ] || [ "$NZ_EXISTS" = false ]; then
            echo "Missing CSV files detected. Downloading from most recent successful workflow run..."

            # Get the most recent successful run ID for this workflow
            RECENT_RUN_ID=$(gh api repos/${{ github.repository }}/actions/runs \
              --jq '.workflow_runs[] | select(.conclusion == "success" and .name == "Broken Link Check") | .id' \
              | head -1)

            if [ -n "$RECENT_RUN_ID" ]; then
              echo "Found recent successful run: $RECENT_RUN_ID"

              # Download artifacts from that run
              ARTIFACT_URL=$(gh api repos/${{ github.repository }}/actions/runs/$RECENT_RUN_ID/artifacts \
                --jq '.artifacts[] | select(.name == "link-check-report") | .archive_download_url')

              if [ -n "$ARTIFACT_URL" ]; then
                echo "Downloading artifacts..."
                gh api "$ARTIFACT_URL" > artifacts.zip

                if [ -f artifacts.zip ]; then
                  unzip -j artifacts.zip "*.csv" 2>/dev/null || echo "No CSV files in artifact"
                  rm artifacts.zip
                  echo "✓ Artifacts processed"
                else
                  echo "⚠ Failed to download artifacts"
                fi
              else
                echo "⚠ No artifacts found in recent run"
              fi
            else
              echo "⚠ No recent successful run found"
            fi
          fi

          # Create placeholder files if still missing
          if [ ! -f "au_link_check_results.csv" ]; then
            echo "Creating placeholder AU CSV file..."
            echo "URL,Status,Response_Time,Error" > au_link_check_results.csv
            echo "https://example.com,200,0.5," >> au_link_check_results.csv
          fi

          if [ ! -f "nz_link_check_results.csv" ]; then
            echo "Creating placeholder NZ CSV file..."
            echo "URL,Status,Response_Time,Error" > nz_link_check_results.csv
            echo "https://example.com,200,0.5," >> nz_link_check_results.csv
          fi

          # Final verification
          echo "Final CSV file status:"
          if [ -f "au_link_check_results.csv" ]; then
            echo "✓ au_link_check_results.csv available ($(wc -l < au_link_check_results.csv) lines)"
          fi

          if [ -f "nz_link_check_results.csv" ]; then
            echo "✓ nz_link_check_results.csv available ($(wc -l < nz_link_check_results.csv) lines)"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Run Product Data Fetch
        run: |
          echo "Starting product data fetch process..."
          python product_data_fetcher.py
        env:
          COMMERCETOOL_AUTH_TOKEN: ${{ secrets.COMMERCETOOL_AUTH_TOKEN }}
          COMMERCETOOL_API_KEY: ${{ secrets.COMMERCETOOL_API_KEY }}
          COMMERCETOOL_PROJECT_KEY: ${{ secrets.COMMERCETOOL_PROJECT_KEY }}
          MAX_PRODUCTS: '100'

      - name: Verify All Required Files Exist
        run: |
          echo "Verifying all required files for report generation..."

          # Check AU CSV
          if [ -f "au_link_check_results.csv" ]; then
            echo "✓ au_link_check_results.csv found ($(wc -l < au_link_check_results.csv) lines)"
          else
            echo "✗ au_link_check_results.csv missing"
            exit 1
          fi

          # Check NZ CSV
          if [ -f "nz_link_check_results.csv" ]; then
            echo "✓ nz_link_check_results.csv found ($(wc -l < nz_link_check_results.csv) lines)"
          else
            echo "✗ nz_link_check_results.csv missing"
            exit 1
          fi

          # Check Product CSV
          PRODUCT_CSV=$(ls product_export_*.csv 2>/dev/null | head -1)
          if [ -n "$PRODUCT_CSV" ]; then
            echo "✓ Product CSV found: $PRODUCT_CSV ($(wc -l < "$PRODUCT_CSV") lines)"
            # Create a standardized name for the report generator
            cp "$PRODUCT_CSV" product_export.csv
          else
            echo "✗ No product export CSV found"
            exit 1
          fi

          echo "All required files are present and ready for report generation"




      - name: Generate Combined Report
        run: python report_generator.py --au-csv au_link_check_results.csv --nz-csv nz_link_check_results.csv --product-csv product_export.csv --output-html combined_report.html

      - name: Prepare GitHub Pages deployment
        run: |
          mkdir -p gh-pages-deploy
          mv combined_report.html gh-pages-deploy/index.html
          cp broken_links.db gh-pages-deploy/ 2>/dev/null || echo "No broken_links.db to copy"
          cp *_link_check_results.csv gh-pages-deploy/ 2>/dev/null || echo "No link check CSV files to copy"
          cp product_export*.csv gh-pages-deploy/ 2>/dev/null || echo "No product export CSV files to copy"

          echo "Files prepared for deployment:"
          ls -la gh-pages-deploy/

      - name: Upload HTML report artifact
        uses: actions/upload-artifact@v4
        with:
          name: link-check-report
          path: |
            gh-pages-deploy/index.html
            gh-pages-deploy/broken_links.db
            gh-pages-deploy/*.csv

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: gh-pages-deploy
          publish_branch: gh-pages
          force_orphan: true
